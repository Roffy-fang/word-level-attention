# word-level-attention
a model that mainly consider the image feature
### Before all
This work is forked from fabulous [neuraltalk2](https://github.com/karpathy/neuraltalk2) from [karparthy](https://github.com/karpathy). And the model implemented here is from "Image Captioning with Word Level attention
" accepted by IEEE International Conference on Image Processingâ€™18
### Overview 
 
![overview](https://github.com/Roffy-fang/word-level-attention/blob/master/coco/fig1.jpg)
### Overview 
Regarding the two folders, coco-cation and coco, and other additional repositories, please refer to the project of [neuraltalk2](https://github.com/karpathy/neuraltalk2)
### License

BSD License.

### Acknowledgements

Parts of this code were written in collaboration with my labmate [Justin Johnson](http://cs.stanford.edu/people/jcjohns/). 

I'm very grateful for [NVIDIA](https://developer.nvidia.com/deep-learning)'s support in providing GPUs that made this work possible.

I'm also very grateful to the maintainers of Torch for maintaining a wonderful deep learning library.
